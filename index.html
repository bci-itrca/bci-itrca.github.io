<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
  
  <!-- Preconnect to external domains for faster loading -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">
  
  <!-- Optimize font loading with display=swap -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&display=swap" rel="stylesheet">

  <!-- Load critical CSS synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Load non-critical CSS asynchronously -->
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" media="print" onload="this.media='all'">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" media="print" onload="this.media='all'">
  
  <!-- Fallback for users with JavaScript disabled -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>


</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="margin-bottom: 0.8rem;">Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs</h1>
            <div class="is-size-5 publication-authors" style="margin-bottom: 0.6rem;">
              <!-- Paper authors -->
              <span class="author-block">
                Ziwen Wang<sup>1</sup>,</span>
                <span class="author-block">
                  Yue Zhang<sup>2</sup>,</span>
                  <span class="author-block">
                    Zhiqiang Zhang<sup>3</sup>,</span>
                  <span class="author-block">
                    Sheng Quan Xie<sup>3</sup>,</span>
                  <span class="author-block">
                    Alexander Lanzon<sup>1</sup>,</span>
                  <span class="author-block">
                    William P. Heath<sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://alvishub.github.io/" target="_blank">Zhenhong Li</a><sup>1,â€ </sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors" style="margin-top: 0.8rem;">
                    <span class="author-block" style="margin-bottom: 0.4rem; display: block;"><sup>1</sup>University of Manchester, <sup>2</sup>University of Bath, <sup>3</sup>University of Leeds, <sup>4</sup>Bangor University</span>
                    <span class="author-block" style="margin-bottom: 0.5rem; display: block;">IEEE Journal of Biomedical and Health Informatics</span>
                    <span class="eql-cntrb"><sup>â€ </sup>Corresponding Author: <a href="mailto:zhenhong.li@manchester.ac.uk">zhenhong.li@manchester.ac.uk</a></span>
                  </div>

                  <div class="column has-text-centered" style="margin-top: 1.2rem;">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/document/11028588" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
<!--                   <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.10933" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces (BCIs) can achieve high recognition accuracy with sufficient training data. Transfer learning presents a promising solution to alleviate data requirements for the target subject by leveraging data from source subjects; however, effectively addressing individual variability among both target and source subjects remains a challenge. This paper proposes a novel transfer learning framework, termed instance-based task-related component analysis (iTRCA), which leverages knowledge from source subjects while considering their individual contributions. iTRCA extracts two types of features: (1) the subject-general feature, capturing shared information between source and target subjects in a common latent space, and (2) the subject-specific feature, preserving the unique characteristics of the target subject. To mitigate negative transfer, we further design an enhanced framework, subject selection-based iTRCA (SS-iTRCA), which integrates a similarity-based subject selection strategy to identify appropriate source subjects for transfer based on their task-related components (TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected dataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA frameworks. This study provides a potential solution for developing high-performance SSVEP-based BCIs with reduced target subject data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- Single Image Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Methodology</h2>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/iTRCA.jpg" alt="Research Methodology"/>
            <div class="content has-text-justified" style="margin-top: 1.5rem; margin-bottom: 1rem;">
              <h6 class="subtitle has-text-centered">
              Overview of the proposed iTRCA framework.
              </h6>
              <p>
                The figure illustrates the proposed iTRCA (instance-based task-related component analysis) framework. For the $i$-th stimulus, the shared information across subjects is captured in a common latent space (the blue block) where source instances and the target subject's individual template are maximally correlated through weight vector $\hat{\mathbf{w}}_i^{GS}$ and the spatial filter $\hat{\mathbf{w}}_i^{GT}$. Meanwhile, target subject's individual information is captured by the spatial filter $\hat{\mathbf{w}}_i^{T_a}$ based on TRCA. In the test stage, the feature $\rho_i$ is composed of the subject-general feature $\rho_{1,i}$ and the subject-specific feature $\rho_{2,i}$, and then combined across all sub-bands to formulate the feature $r_i$ used for SSVEP recognition.
              </p>
              <p>
                iTRCA computes the task-related components (TRCs) of source subjects as source instances and then projects them into a latent space, where these source instances and the target subject's template exhibit maximal correlation. This latent space captures the underlying characteristics of SSVEP responses shared between source and target subjects, while accounting for the varying contributions of individual source subjects. Meanwhile, iTRCA captures target subject's individual characteristics using TRCA.
              </p>
              <p>
                <strong>However, not all source subjects contribute positively to the target subject's performance. </strong>  When the knowledge learned from source subjects have a detrimental effect on the target subject (e.g., reducing recognition accuracy), the transfer is regarded as negative transfer. To address this issue, the proposed iTRCA framework is further enhanced by the subject selection-based iTRCA (SS-iTRCA), which integrates a similarity-based subject selection strategy to identify appropriate source subjects for transfer based on their TRCs. This approach mitigates negative transfer by ensuring that only relevant source subjects contribute to the transfer learning process.
              </p>
            </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End single image section -->



<!-- Experimental Results Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Main Results</h2>
          <div class="item">
            <!-- Multi-subfigure layout -->
            <div class="columns is-multiline is-centered">
              <div class="column is-one-third">
                <figure class="image">
                  <img src="static/images/TRCA_iTRCA_SS@tw_benchmark-eps-converted-to_1.png" alt="Benchmark Dataset Results"/>
                  <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                    <strong>(a) Benchmark</strong>
                  </figcaption>
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image">
                  <img src="static/images/TRCA_iTRCA_SS@tw_beta-eps-converted-to_1.png" alt="BETA Dataset Results"/>
                  <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                    <strong>(b) BETA</strong>
                  </figcaption>
                </figure>
              </div>
              <div class="column is-one-third">
                <figure class="image">
                  <img src="static/images/TRCA_iTRCA_SS@tw_yue-eps-converted-to_1.jpg" alt="Self-collected Dataset Results"/>
                  <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                    <strong>(c) Self-collected</strong>
                  </figcaption>
                </figure>
              </div>
            </div>
            
            <div class="content has-text-justified" style="margin-top: 2rem;">
              <p class="has-text-centered" style="font-style: italic; margin-bottom: 1.5rem;">
                <strong>Figure 1.</strong> The average recognition accuracy and ITR for TRCA, iTRCA and SS-iTRCA across all subjects for different $d$ on three datasets: (a) Benchmark, (b) BETA, and (c) Self-collected. Error bars represent SEM. Asterisks indicate the statistically significant difference between two algorithms ($*: p<0.05$, $**: p<0.01$, $***: p<0.001$, $****: p<0.0001$). Accuracy and ITR are analyzed by two-way and one-way repeated-measures ANOVA, respectively.
              </p>
              <p>
                The figure shows the average accuracy and ITR across all subjects for $d$ ranging from 0.2 s to 1 s in 0.2 s increments, with $N_c = 9$, $N_{tb} = 3$, and $c_{lb} = 0.9$.
                Across all datasets, recognition accuracy of all algorithms increases monotonically with $d$, suggesting that longer data segments provide more reliable information for recognition when $d<1$ s.
              </p>
              <p>
                On public datasets (Benchmark and BETA), iTRCA outperforms TRCA from 0.4 s onward, with SS-iTRCA further improving accuracy. However, the extent of improvement varies: on BETA, SS-iTRCA shows a more pronounced accuracy enhancement over iTRCA as $d$ increases, whereas on Benchmark, the accuracy difference is less significant. On the self-collected dataset, iTRCA consistently outperforms TRCA across all $d$ values, while SS-iTRCA achieves comparable accuracy to iTRCA, with no statistically significant differences observed. Regarding ITR, the results across all datasets show a consistent pattern: ITR increases rapidly from 0.2 s to 0.6 s, peaks around 0.8 s, and either stabilizes or slightly declines at 1 s. Unlike the monotonic increase in accuracy, ITR exhibits a growth-and-decline trend as $d$ increases, reflecting the trade-off between recognition speed and accuracy: longer $d$ enhances accuracy but also reduces speed, potentially lowering overall transfer efficiency.

                On public datasets, SS-iTRCA achieves the highest ITR from 0.4 s onward, TRCA the lowest, and iTRCA lies in between. On the self-collected dataset, SS-iTRCA and iTRCA show comparable ITR performance, while TRCA consistently lags behind across all $d$ values.
              </p>        
            </div>
            <!-- Subject Selection Analysis - Figure 2 -->
            <div style="margin-top: 1.5rem;">
              
              <div class="item">
                <!-- Multi-subfigure layout for subject selection -->
                <div class="columns is-multiline is-centered">
                  <!-- First row: two subfigures -->
                  <div class="column is-half">
                    <figure class="image">
                      <img src="static/images/ss_clb-eps-converted-to_1.jpg" alt="Average recognition accuracy across all subjects"/>
                      <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                        <strong>(a)</strong> Average recognition accuracy across all subjects
                      </figcaption>
                    </figure>
                  </div>
                  <div class="column is-half">
                    <figure class="image">
                      <img src="static/images/ss_Nsrc-eps-converted-to_1.jpg" alt="Number of selected source subjects"/>
                      <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                        <strong>(b)</strong> Number of selected source subjects
                      </figcaption>
                    </figure>
                  </div>
                  <!-- Second row: one centered subfigure -->
                  <div class="column is-two-thirds">
                    <figure class="image">
                      <img src="static/images/ss_allsubs-eps-converted-to_1.jpg" alt="Selected source subjects for each target subject"/>
                      <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                        <strong>(c)</strong> Selected source subjects for each target subject at different thresholds
                      </figcaption>
                    </figure>
                  </div>
                </div>
                
                <div class="content has-text-justified" style="margin-top: 2rem;">
                  <p class="has-text-centered" style="font-style: italic; margin-bottom: 1.5rem;">
                    <strong>Figure 2.</strong> Subject selection performance on Benchmark. (a) Average recognition accuracy across all subjects. (b) The number of selected source subjects where the scatter point represents the target subject and the gray bar represents the average number across 35 target subjects. (c) The number of selected source subjects for each target subject at different thresholds $c_{lb}$. For example, for target subject 1, the number of selected source subjects is 15, 12, 10, 7, and 3 at $c_{lb}=$ 0.5, 0.6, 0.7, 0.8, and 0.9, respectively. For target subject 11, all source subjects are included, corresponding to the case where the subject selection strategy is not triggered.
                  </p>
                  
                  <p>
                    Figure 2(a) reveals the occurrence of negative transfer. As $c_{lb}$ decreases ($c_{lb} \leq 0.8$), more source subjects with lower similarity to the target subject are included, leading to a decline in accuracy. This finding coincides with the negative transfer phenomenon, where dissimilar source data can hurt the target task (target subject's SSVEP recognition in this study), highlighting the importance of source subject selection. Moreover, it demonstrates that the designed TRC-based similarity metric can reflect the similarity between source and target subjects, thereby improving recognition accuracy even with fewer, but more relevant, source subjects.
                  </p>
                  
                </div>
              </div>
            </div>
            
            <!-- Transfer Learning Comparison - Figure 3 -->
            <div style="margin-top: 3rem;">

              <div class="item">
                <!-- Two subfigure layout for transfer learning comparison -->
                <div class="columns is-centered">
                  <div class="column is-half">
                    <figure class="image">
                      <img src="static/images/TF_subfigure_accuracy-eps-converted-to_1.png" alt="Recognition accuracy comparison"/>
                      <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                        <strong>(a)</strong> Recognition accuracy
                      </figcaption>
                    </figure>
                  </div>
                  <div class="column is-half">
                    <figure class="image">
                      <img src="static/images/TF_subfigure_computationCost-eps-converted-to_1.jpg" alt="Computation cost comparison"/>
                      <figcaption class="has-text-centered" style="margin-top: 0.5rem;">
                        <strong>(b)</strong> Computation cost
                      </figcaption>
                    </figure>
                  </div>
                </div>
                
                <div class="content has-text-justified" style="margin-top: 2rem;">
                  <p class="has-text-centered" style="font-style: italic; margin-bottom: 1.5rem;">
                    <strong>Figure 3.</strong> Performance comparison between the proposed methods (SS-iTRCA and iTRCA) with existing transfer learning methods (transRCA, DGTF, and CSSFT). (a) Recognition accuracy, with a paired t-test conducted between iTRCA and transRCA. (b) Training and inference time for each method, where the test time corresponds to a block comprising 40 test trials in the benchmark dataset.
                  </p>
                  
                  <p>
                   All experiments are conducted using MATLAB R2022a on a Windows 11 system, equipped with a 13th Intel Core i7-13700H CPU running at 2.40 GHz and an NVIDIA GeForce RTX 4070 Laptop GPU. The proposed frameworks consistently outperforms TransRCA, DGTF, and CSSFT across all $d$ values, with statistically significant differences. For both training and inference stages, SS-iTRCA, iTRCA and TransRCA exhibit significantly lower computational costs compared to CSSFT and DGTF. Although TransRCA achieves shorter training time than SS-iTRCA and iTRCA, there is no statistically significant difference in inference time among them, which is crucial for real-time signal recognition. Compared with accuracy-based subject selection method CSSFT, the proposed similarity-based subject selection strategy SS-iTRCA substantially reduces the computation cost.
                  </p>
                
                </div>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End experimental results section -->








<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->







<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<video width="640" height="360" controls>
  <source src="static/videos/video-source.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title has-text-centered">BibTeX</h2>
      <pre><code>@ARTICLE{11028588,
  author={Wang, Ziwen and Zhang, Yue and Zhang, Zhiqiang and Xie, Sheng Quan and Lanzon, Alexander and Heath, William P. and Li, Zhenhong},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs}, 
  year={2025},
  volume={},
  number={},
  pages={1-11},
  doi={10.1109/JBHI.2025.3577813}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Lab Logo Section -->
<section class="section" style="margin-top: -6rem;">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <h2 class="title is-4"></h2>
          <figure class="image is-inline-block" style="max-width: 300px;">
            <a href="https://neurorobotics-uom.github.io/" target="_blank">
              <img src="static/images/lab_logo_black2.png" alt="Lab Logo" style="max-height: 125px; width: auto;">
            </a>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End Lab Logo Section -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> 
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <!-- Move all JavaScript to bottom for better loading performance -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js" defer></script>
    <!-- Removed unused Adobe PDF viewer script for better performance -->
    <!-- <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script> -->
    <script src="static/js/fontawesome.all.min.js" defer></script>
    <script src="static/js/bulma-carousel.min.js" defer></script>
    <script src="static/js/bulma-slider.min.js" defer></script>
    <script src="static/js/index.js" defer></script>
    
    <!-- LOCAL MathJax for faster loading -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        },
        startup: {
          ready: function () {
            MathJax.startup.defaultReady();
            // Process math after page loads
            MathJax.typesetPromise();
          }
        }
      };
    </script>
    <script src="static/js/mathjax/es5/tex-mml-chtml.js"></script>
    
    <!-- LOCAL Marked.js for faster loading -->
    <script src="static/js/marked.min.js"></script>
    
    <script>
      // Render Markdown content when page loads
      document.addEventListener('DOMContentLoaded', function() {
        const markdownElements = document.querySelectorAll('.markdown-content');
        markdownElements.forEach(function(element) {
          const markdownText = element.getAttribute('data-markdown');
          if (markdownText) {
            element.innerHTML = marked.parse(markdownText);
          }
        });
        
        // Re-render MathJax after Markdown is processed
        if (window.MathJax && window.MathJax.typesetPromise) {
          MathJax.typesetPromise();
        }
      });
    </script>

  </body>
  </html>
